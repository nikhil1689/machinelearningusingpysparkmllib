{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLlib is a component of Apache Spark for machine learning\n",
    "\n",
    "Various tools provided by MLlib include:\n",
    "    \n",
    "ML Algorithms: collaborative filtering, classification and clustering\n",
    "    \n",
    "Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "    \n",
    "Pipelines: tools for constructing, evaluating, and tuning ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Scikit-learn is a popular Python library for data mining and machine learning\n",
    "\n",
    "Scikit-learn algorithms only work for small datasets on a single machine\n",
    "\n",
    "Spark's MLlib algorithms are designed for parallel processing on a cluster\n",
    "\n",
    "Supports languages such as Scala, Java, and R\n",
    "\n",
    "Provides a high-level API to build machine learning pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The three C's of machine learning in PySpark MLlib\n",
    "\n",
    "Collaborative Filtering: Produce recommendations, useful in recommendation engines\n",
    "\n",
    "Classification: Which of a set categorises a new observation\n",
    "    \n",
    "Clustering: Grouping data based on similar charateristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pyspark.mllib.recommendation\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "#pyspark.mllib.classification\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "#pyspark.mllib.clustering\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "Finding users that share common interests\n",
    "\n",
    "It can be user based or item based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rating class in pyspark.mllib.recommendation submodule\n",
    "\n",
    "The Rating class is a wrapper around tuple (user, product and rating)\n",
    "\n",
    "Useful for parsing the RDD and creating a tuple of user id, product id and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "r = Rating(user = 1, product = 2, rating = 5.0)\n",
    "(r[0], r[1], r[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Splitting the data using randomSplit()\n",
    "\n",
    "Between training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "training, test=data.randomSplit([0.6, 0.4])\n",
    "training.collect()\n",
    "test.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Least Squares (ALS)\n",
    "\n",
    "Alternating Least Squares (ALS) algorithm in spark.mllib provides collaborative filtering\n",
    "\n",
    "ALS.train(ratings, rank, iterations)\n",
    "\n",
    "rank means number of features\n",
    "\n",
    "number of iterations to run least square computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r1 = Rating(1, 1, 1.0)\n",
    "r2 = Rating(1, 2, 2.0)\n",
    "r3 = Rating(2, 1, 2.0)\n",
    "ratings = sc.parallelize([r1, r2, r3])\n",
    "ratings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ALS.train(ratings, rank=10, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictAll() â€“ Returns RDD of Rating Objects\n",
    "\n",
    "The predictAll() method returns a list of predicted ratings for input user and product pair\n",
    "\n",
    "The method takes in an RDD without ratings to generate the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrated_RDD = sc.parallelize([(1, 2), (1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predictAll(unrated_RDD)\n",
    "predictions.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flights = pd.read_csv('flights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mon  dom  dow carrier  flight  org  mile  depart  duration  delay\n",
      "0   11   20    6      US      19  JFK  2153    9.48       351    NaN\n",
      "1    0   22    2      UA    1107  ORD   316   16.33        82   30.0\n",
      "2    2   20    4      UA     226  SFO   337    6.17        82   -8.0\n",
      "3    9   13    1      AA     419  ORD  1236   10.33       195   -5.0\n",
      "4    4    2    5      AA     325  ORD   258    8.92        65    NaN\n"
     ]
    }
   ],
   "source": [
    "print(flights.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the 'flight' column\n",
    "flights = flights.drop(columns='flight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mon  dom  dow carrier  org  mile  depart  duration  delay\n",
      "0   11   20    6      US  JFK  2153    9.48       351    NaN\n",
      "1    0   22    2      UA  ORD   316   16.33        82   30.0\n",
      "2    2   20    4      UA  SFO   337    6.17        82   -8.0\n",
      "3    9   13    1      AA  ORD  1236   10.33       195   -5.0\n",
      "4    4    2    5      AA  ORD   258    8.92        65    NaN\n"
     ]
    }
   ],
   "source": [
    "print(flights.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flights=flights.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mon  dom  dow carrier  org  mile  depart  duration  delay\n",
      "1    0   22    2      UA  ORD   316   16.33        82   30.0\n",
      "2    2   20    4      UA  SFO   337    6.17        82   -8.0\n",
      "3    9   13    1      AA  ORD  1236   10.33       195   -5.0\n",
      "5    5    2    1      UA  SFO   550    7.98       102    2.0\n",
      "6    7    2    6      AA  ORD   733   10.83       135   54.0\n"
     ]
    }
   ],
   "source": [
    "print(flights.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "flights = spark.createDataFrame(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Derive a new km column from the mile column, rounding to zero decimal places. One mile is 1.60934 km.\n",
    "\n",
    "#Remove the mile column.\n",
    "\n",
    "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
    "                    .drop('mile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a label column with a value of 1 indicating the delay was 15 minutes or more and 0 otherwise.\n",
    "\n",
    "flights = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82| 30.0| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82| -8.0| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195| -5.0|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|  2.0| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135| 54.0|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check first five records\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In the flights data there are two columns, carrier and org, which hold categorical data. \n",
    "\n",
    "You need to transform those columns into indexed numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer object to transform the carrier column from a string to an numeric index.\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data, Prepare the indexer object on the flight data.\n",
    "indexer_model = indexer.fit(flights)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82| 30.0| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82| -8.0| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195| -5.0|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|  2.0| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135| 54.0|1180.0|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling columns\n",
    "\n",
    "The final stage of data preparation is to consolidate all of the predictor columns into a single column.\n",
    "\n",
    "An updated version of the flights data, which takes into account all of the changes from the previous few exercises, has the following predictor columns:\n",
    "\n",
    "mon, dom and dow\n",
    "carrier_idx (indexed value from carrier)\n",
    "org_idx (indexed value from org)\n",
    "km\n",
    "depart\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[0.0,22.0,2.0,16.33,82.0,509.0,0.0,0.0]  |30.0 |\n",
      "|[2.0,20.0,4.0,6.17,82.0,542.0,0.0,1.0]   |-8.0 |\n",
      "|[9.0,13.0,1.0,10.33,195.0,1989.0,1.0,0.0]|-5.0 |\n",
      "|[5.0,2.0,1.0,7.98,102.0,885.0,0.0,1.0]   |2.0  |\n",
      "|[7.0,2.0,6.0,10.83,135.0,1180.0,1.0,0.0] |54.0 |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow','depart', 'duration','km','carrier_idx', 'org_idx'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights = assembler.transform(flights)\n",
    "\n",
    "# Check the resulting column\n",
    "flights.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "\n",
    "# For repeatability set a random number seed of 17 for the split.\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47022"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37592"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37592, 9430]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[flights_train.count(),flights_test.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+--------------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|            features|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+--------------------+\n",
      "|  0|  1|  2|     AA|LGA| 20.42|     185| 31.0|1765.0|    1|        1.0|    3.0|[0.0,1.0,2.0,20.4...|\n",
      "|  0|  1|  2|     AA|ORD|  8.75|     120|  8.0|1180.0|    0|        1.0|    0.0|[0.0,1.0,2.0,8.75...|\n",
      "|  0|  1|  2|     AA|ORD| 11.08|     105| 18.0| 985.0|    1|        1.0|    0.0|[0.0,1.0,2.0,11.0...|\n",
      "|  0|  1|  2|     AA|ORD| 12.67|     160| 68.0|1429.0|    1|        1.0|    0.0|[0.0,1.0,2.0,12.6...|\n",
      "|  0|  1|  2|     AA|ORD|  15.0|      70| 62.0| 415.0|    1|        1.0|    0.0|[0.0,1.0,2.0,15.0...|\n",
      "|  0|  1|  2|     AA|ORD| 16.75|     260|137.0|2778.0|    1|        1.0|    0.0|[0.0,1.0,2.0,16.7...|\n",
      "|  0|  1|  2|     AA|ORD| 17.75|     180|237.0|1926.0|    1|        1.0|    0.0|[0.0,1.0,2.0,17.7...|\n",
      "|  0|  1|  2|     AA|ORD|  19.0|      80| 61.0| 378.0|    1|        1.0|    0.0|[0.0,1.0,2.0,19.0...|\n",
      "|  0|  1|  2|     AA|ORD|  19.5|     130| 63.0|1180.0|    1|        1.0|    0.0|[0.0,1.0,2.0,19.5...|\n",
      "|  0|  1|  2|     AA|ORD| 20.25|     275| 34.0|2971.0|    1|        1.0|    0.0|[0.0,1.0,2.0,20.2...|\n",
      "|  0|  1|  2|     AA|ORD| 21.08|      85| 29.0| 649.0|    1|        1.0|    0.0|[0.0,1.0,2.0,21.0...|\n",
      "|  0|  1|  2|     AA|ORD| 21.92|      64| 62.0| 378.0|    1|        1.0|    0.0|[0.0,1.0,2.0,21.9...|\n",
      "|  0|  1|  2|     AA|SFO| 23.42|     325| 22.0|4352.0|    1|        1.0|    1.0|[0.0,1.0,2.0,23.4...|\n",
      "|  0|  1|  2|     AQ|OGG|  7.08|      34| -2.0| 161.0|    0|        8.0|    7.0|[0.0,1.0,2.0,7.08...|\n",
      "|  0|  1|  2|     AQ|OGG|  15.0|     315| 43.0|4089.0|    1|        8.0|    7.0|[0.0,1.0,2.0,15.0...|\n",
      "|  0|  1|  2|     B6|JFK| 15.42|     237| -8.0|2570.0|    0|        4.0|    2.0|[0.0,1.0,2.0,15.4...|\n",
      "|  0|  1|  2|     B6|JFK| 19.17|     200| 33.0|1617.0|    1|        4.0|    2.0|[0.0,1.0,2.0,19.1...|\n",
      "|  0|  1|  2|     B6|JFK| 20.58|     204| 79.0|1728.0|    1|        4.0|    2.0|[0.0,1.0,2.0,20.5...|\n",
      "|  0|  1|  2|     B6|SFO|   8.0|     323|-10.0|4162.0|    0|        4.0|    1.0|[0.0,1.0,2.0,8.0,...|\n",
      "|  0|  1|  2|     HA|OGG| 19.92|      34|-11.0| 161.0|    0|        7.0|    7.0|[0.0,1.0,2.0,19.9...|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_model = tree.fit(flights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|label|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|1    |1.0       |[0.37313780260707635,0.6268621973929237]|\n",
      "|1    |0.0       |[0.5275247524752475,0.47247524752475245]|\n",
      "|1    |1.0       |[0.43974159055468925,0.5602584094453108]|\n",
      "|1    |1.0       |[0.43974159055468925,0.5602584094453108]|\n",
      "|1    |1.0       |[0.43974159055468925,0.5602584094453108]|\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A confusion matrix gives a useful breakdown of predictions versus known values. \n",
    "It has four cells which represent the counts of:\n",
    "\n",
    "True Negatives (TN) â€” model predicts negative outcome & known outcome is negative\n",
    "True Positives (TP) â€” model predicts positive outcome & known outcome is positive\n",
    "False Negatives (FN) â€” model predicts negative outcome but known outcome is positive\n",
    "False Positives (FP) â€” model predicts positive outcome but known outcome is negative.\n",
    "\n",
    "The accuracy is the ratio of correct predictions (TP and TN) to all predictions (TP, TN, FP and FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1664|\n",
      "|    0|       0.0| 2596|\n",
      "|    1|       1.0| 3130|\n",
      "|    0|       1.0| 2040|\n",
      "+-----+----------+-----+\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix by counting the combinations of label and prediction\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6344391785150079"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2279+3745)/(1079+2279+3745+2392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "\n",
    "An ensemble model combines the results from multiple models to produce better predictions than any one of those models acting alone. \n",
    "\n",
    "The concept is based on the idea of the \"Wisdom of the Crowd\", which implies that the aggregated opinion of a group is better than the opinions of the individuals in that group, even if the individuals are experts.\n",
    "\n",
    "Ideally each of the models in the ensemble should be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "\n",
    "- each tree is trained on a different random subset of the data and \n",
    "\n",
    "- within each tree a random subset of features is used for splitting at each node. \n",
    "\n",
    "The result is a collection of trees where no two trees are the same. \n",
    "\n",
    "Within the Random Forest model, all of the trees operate in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|             1|    1|[0.0,1.0,2.0,13.7...|\n",
      "|             0|    1|[0.0,1.0,2.0,6.75...|\n",
      "|             1|    1|[0.0,1.0,2.0,17.5...|\n",
      "|             1|    1|[0.0,1.0,2.0,20.2...|\n",
      "|             1|    1|[0.0,1.0,2.0,20.5...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(flights)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(flights)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(flights_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(flights_test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The objective is to predict\n",
    "\n",
    "whether a flight is likely to be delayed by at least 15 minutes (label 1) or not (label 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "you'll only use the mon, depart and duration columns for the moment. \n",
    "\n",
    "These are numerical features which can immediately be used for a Logistic Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The data have been split into training and testing sets and \n",
    "\n",
    "are available as flights_train and flights_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1664|\n",
      "|    0|       0.0| 2596|\n",
      "|    1|       1.0| 3130|\n",
      "|    0|       1.0| 2040|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Evaluate the Logistic Regression model\n",
    "\n",
    "Accuracy is generally not a very reliable metric because it can be biased by the most common target \n",
    "\n",
    "class.\n",
    "\n",
    "There are two other useful metrics:\n",
    "\n",
    "precision and\n",
    "recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Precision is the proportion of positive predictions which are correct. \n",
    "For all flights which are predicted to be delayed, what proportion is actually delayed?\n",
    "\n",
    "Recall is the proportion of positives outcomes which are correctly predicted. \n",
    "For all delayed flights, what proportion is correctly predicted by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.61\n",
      "recall    = 0.65\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Encoding flight origin\n",
    "The org column in the flights data is a categorical variable giving the airport \n",
    "from which a flight departs.\n",
    "\n",
    "ORD â€” O'Hare International Airport (Chicago)\n",
    "SFO â€” San Francisco International Airport\n",
    "JFK â€” John F Kennedy International Airport (New York)\n",
    "LGA â€” La Guardia Airport (New York)\n",
    "SMF â€” Sacramento\n",
    "SJC â€” San Jose\n",
    "TUS â€” Tucson International Airport\n",
    "OGG â€” Kahului (Hawaii)\n",
    "Obviously this is only a small subset of airports. \n",
    "Nevertheless, since this is a categorical variable, \n",
    "it needs to be one-hot encoded before it can be used in a regression model.\n",
    "\n",
    "The data are in a variable called flights. \n",
    "You have already used a string indexer to create a column of indexed values \n",
    "corresponding to the strings in org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Flight duration model: Just distance\n",
    "In this exercise you'll build a regression model to predict flight duration (the duration column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|155     |155.00000000000006|\n",
      "|341     |341.00000000000017|\n",
      "|155     |155.00000000000006|\n",
      "|157     |157.0             |\n",
      "|114     |114.0             |\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0811031774548243e-13"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(featuresCol='features',labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.43905683111e-13\n",
      "[-9.5374281055128e-16,-7.291202184260976e-16,-2.5178785244852695e-15,-1.4398988386382168e-15,1.0000000000000067,-5.070569873311002e-16,-1.5725358249002988e-14,1.3377064015779953e-14]\n",
      "-9.5374281055128e-16\n",
      "-6.291004171797526e+16\n"
     ]
    }
   ],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0, -0.0, -0.0, -0.0, 1.0, -0.0, -0.0, 0.0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.4390568311079943e-13"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
